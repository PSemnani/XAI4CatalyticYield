{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "245705bf-d3ae-4a83-b647-5bfa36b32890",
   "metadata": {},
   "source": [
    "# Evaluate ML framework for neural network, explanations using LRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import timedelta\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import mlxai4cat.utils.LRP_tools as LRP\n",
    "from mlxai4cat.models.neural_network import NeuralNetwork, ModifiedNeuralNetwork\n",
    "from mlxai4cat.utils.nn_training import train_epoch, val_epoch\n",
    "from mlxai4cat.utils.data import prepare_dataset, stratified_sampling, resampling, get_test_data_loader, get_xval_data_loaders\n",
    "from mlxai4cat.utils.visualization import get_formatted_results, plot_feature_importance, plot_feature_importance_distribution, custom_palette\n",
    "from mlxai4cat.models.generative import generate_catalysts_from_relevance_scores, catalyst_string_to_numpy, numpy_to_catalyst_string\n",
    "from mlxai4cat.utils.LRP_tools import LRPAnalyzer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2076b4-6b7a-46a4-8644-1023658f0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4750b10",
   "metadata": {},
   "source": [
    "### Storing information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path = Path('../results')\n",
    "figure_path = Path('../figures')\n",
    "SAVE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2359e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0063123",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X, y, X_pos, y_pos, X_neg, y_neg, feature_names = prepare_dataset('../data/ocm_cat_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470f8fc",
   "metadata": {},
   "source": [
    "## Cross validation setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = [2, 3, 4]\n",
    "num_neurons_per_layers = {\n",
    "    1: [[36, 16, 2], [36, 32, 2], [36, 64, 2], [36, 128, 2]], \n",
    "    2:[[36, 16, 16, 2], [36, 32, 32, 2], [36, 64, 64, 2], [36, 128, 128, 2]],\n",
    "    3: [[36, 16, 16, 16, 2], [36, 32, 32, 32, 2], [36, 64, 64, 64, 2], [36, 128, 128, 128, 2]]\n",
    "}\n",
    "dropout_rates = [0, 0.1]\n",
    "#dropout_rates = [0, 0.05, 0.1, 0.15, 0.2]\n",
    "#lr = [1e-2, 1e-3, 1e-4, 1e-5]\n",
    "#wd = [0, 1e-4, 1e-5]\n",
    "lr = [1e-3]\n",
    "wd = [0.5e-2]\n",
    "all_combs = []\n",
    "\n",
    "for i in range(len(num_layers)):\n",
    "    for v in list(num_neurons_per_layers.values())[i]:\n",
    "        for p in dropout_rates:\n",
    "            for l in lr:\n",
    "                for w in wd:\n",
    "                    combs = []\n",
    "                    combs.append(num_layers[i])\n",
    "                    combs.append(v)\n",
    "                    combs.append(p)\n",
    "                    combs.append(l)\n",
    "                    combs.append(w)\n",
    "                    all_combs.append(combs)\n",
    "\n",
    "print(f\"All combinations ({len(all_combs)}): \\n{all_combs}\")\n",
    "\n",
    "#all_combs = all_combs[0:50]  # TODO: remove it was just for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74171847",
   "metadata": {},
   "source": [
    "## Neural network with resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1871d3-96d8-4319-ab26-862dc13fbcd3",
   "metadata": {},
   "source": [
    "### Explanations based on positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35296aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_resampling = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c4a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This explanation is based on the positive class [0,1]\n",
    "criterion = nn.BCELoss()\n",
    "acc = 0\n",
    "k = 5 # cross-validation folds\n",
    "n = 100\n",
    "n_iter = 20\n",
    "patience = 10  \n",
    "verbose = False\n",
    "# instantiate variables for ease of use later\n",
    "artificial_neuron=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12808aae-9e7a-4f44-b249-fa3e7e10ad36",
   "metadata": {},
   "source": [
    "### Training and nested cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mlp_g = []\n",
    "precision_mlp_g = []\n",
    "recall_mlp_g = []\n",
    "f1_mlp_g = []\n",
    "\n",
    "\n",
    "selected_models_max_g = defaultdict(list)  # For storing the selected model architecture and its F1 score for each split\n",
    "selected_models_g = defaultdict(list)  # For storing the all model architecture and its F1 score for each split\n",
    "selected_models_counts_g = Counter()  # For storing counts and F1 scores of selected models across splits\n",
    "selected_models = [] # store best models as objects\n",
    "split_test_data = {}  # Store confusion information and relevance scores for each split\n",
    "\n",
    "for rs in range(n):\n",
    "    print(f\"Split {rs}\")\n",
    "    start_time = time.time()\n",
    "    ### Get the data loaders for the current split\n",
    "\n",
    "    X_train, y_train, test_loader = get_test_data_loader(X_pos, X_neg, y_pos, y_neg, rs)\n",
    "\n",
    "    ### Iterate over all combinations of hyperparameters for given split and select the best-performing model\n",
    "    max_f1_model = None  # Model with max validation F1 score\n",
    "    max_f1_val = 0  # Max validation F1 score\n",
    "\n",
    "    convergences = []\n",
    "    idx_max_f1_model = 0\n",
    "    for c, comb in enumerate(all_combs[:5]):\n",
    "        if c % 50 == 0:\n",
    "            print(f\"> Combination {c}/{len(all_combs)}\")\n",
    "\n",
    "        num_layers, num_neurons_per_layer, dropout_rate, lr, wd = comb\n",
    "\n",
    "        model = NeuralNetwork(num_layers, num_neurons_per_layer, dropout_rate, artificial_neuron=act_rel)\n",
    "        if torch.cuda.is_available() and use_gpu:\n",
    "            model = model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay=wd, lr=lr)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        xval_f1_scores = []\n",
    "\n",
    "        for k_i in range(k):\n",
    "            train_loader, val_loader = get_xval_data_loaders(k, k_i, X_train, y_train,\n",
    "                                                             with_resampling=with_resampling,\n",
    "                                                             verbose=verbose)\n",
    "            early_stopping_counter = 0\n",
    "            best_val_f1 = 0\n",
    "            f1_val = 0\n",
    "            iteration = 0\n",
    "\n",
    "            # iterate n_iter epochs\n",
    "            for iteration in tqdm(range(n_iter), desc=f\"Training for max {n_iter} epochs\", leave=False):\n",
    "\n",
    "                model.train()\n",
    "                train_epoch(train_loader, model, criterion, optimizer)\n",
    "\n",
    "                model.eval()\n",
    "                val_pred, val_gt = val_epoch(val_loader, model)\n",
    "        \n",
    "                f1_val = f1_score(np.array(val_gt), np.array(val_pred))\n",
    "        \n",
    "                if f1_val > best_val_f1:\n",
    "                    best_val_f1 = f1_val\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "        \n",
    "                if early_stopping_counter >= patience:\n",
    "                    break\n",
    "            xval_f1_scores.append(best_val_f1)\n",
    "        \n",
    "        best_val_f1 = np.mean(xval_f1_scores)\n",
    "\n",
    "        # Store epoch that the model converged\n",
    "        convergences.append(iteration)\n",
    "        # Store the selected model architecture and its F1 score for this split\n",
    "        model_architecture = str(model)  # Convert the model architecture to a string\n",
    "        selected_models_g[model_architecture].append({'f1_score': f1_val, 'hyperparams': comb})\n",
    "\n",
    "        # Store the model for given combination if it has a higher F1 score\n",
    "        if best_val_f1 >= max_f1_val:\n",
    "            max_f1_model = copy.deepcopy(model)\n",
    "            max_f1_val = best_val_f1\n",
    "            max_f1_comb = comb\n",
    "            idx_max_f1_model = c\n",
    "\n",
    "    ### Store model architecture that achieved the highest F1 score over all combinations for 1 split\n",
    "    model_architecture_max = str(max_f1_model)  # Convert the model architecture to a string\n",
    "    selected_models_max_g[model_architecture_max].append({\"f1_score\": max_f1_val, \"hyperparams\": max_f1_comb})\n",
    "    selected_models.append((max_f1_val, max_f1_model))\n",
    "    # Increase counter for the selected model architecture\n",
    "    selected_models_counts_g[model_architecture_max] += 1\n",
    "    max_f1_model.cpu()\n",
    "\n",
    "    ### Evaluate the best-performing model on the test set.\n",
    "    max_f1_model.eval()\n",
    "    modified_model = ModifiedNeuralNetwork(max_f1_model)\n",
    "\n",
    "    pred, gt, probs, rels, confusion_scores, confusion_idxs = modified_model.inference_with_relevance(test_loader, reweight_explanation=True,\n",
    "                                                                                               relevance_on_positive_class=True)\n",
    "    gt = np.array(gt)\n",
    "    pred = np.array(pred)\n",
    "    rels = np.array(rels)\n",
    "    probs = np.array(probs)\n",
    "\n",
    "    split_test_data[rs] = {\n",
    "        'pred': pred,\n",
    "        'gt': gt,\n",
    "        'rels': rels,\n",
    "        'probs' : probs,\n",
    "        # rels is a dict, containing the keys R_on_pred and R_on_pos_cls, each of which is a list of numpy arrays\n",
    "        'confusion_scores': confusion_scores,\n",
    "        # confusion_scores is a dict, containing the keys true_pos_scores, true_neg_scores, false_pos_scores, and false_neg_scores, each of which is a list of floats\n",
    "        'confusion_idxs': confusion_idxs,\n",
    "        # confusion_idxs is a dict, containing the keys true_pos_idx, true_neg_idx, false_pos_idx, and false_neg_idx, each of which is a list of integers\n",
    "        'convergence_times': convergences,\n",
    "        # store number of epochs until convergence for each combination of hyperparameters\n",
    "        'convergence_time_max_f1_model': convergences[idx_max_f1_model]\n",
    "        # store number of epochs until convergence for the model with the highest F1 score\n",
    "    }\n",
    "\n",
    "    acc_mlp_g.append(accuracy_score(gt, pred))\n",
    "    precision_mlp_g.append(precision_score(gt, pred, zero_division=1))\n",
    "    recall_mlp_g.append(recall_score(gt, pred))\n",
    "    f1_mlp_g.append(f1_score(gt, pred))\n",
    "\n",
    "    print(f\"> Total computation time for split {rs}: {timedelta(seconds=(time.time() - start_time))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f623221-ddcb-4418-babe-e5d762ffaeb5",
   "metadata": {},
   "source": [
    "### Store all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    with open(storing_path / f'all_results_resampling_{with_resampling}.pkl', 'wb') as f:\n",
    "        pickle.dump(split_test_data, f)\n",
    "        \n",
    "    with open(storing_path / f'max_f1_models_resampling_{with_resampling}.pkl', 'wb') as f:\n",
    "        pickle.dump(selected_models_max_g, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513867d2-dd6a-48ef-94a6-650dbe4fea6f",
   "metadata": {},
   "source": [
    "### Display and save the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37855399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics = get_formatted_results(acc_mlp_g, f1_mlp_g, precision_mlp_g, recall_mlp_g, 'Neural Networks', verbose=True)\n",
    "if SAVE:\n",
    "    file_path = f'mlp_metrics_results_csv'\n",
    "    ## SAVING ANALYSIS RESULTS\n",
    "    df_metrics.to_csv(os.path.join(storing_path, 'NN_metrics_results.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45f3cc-cdce-44a9-8407-102c069f3098",
   "metadata": {},
   "source": [
    "### Signed and absolute average feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d425e-3262-4ae7-bfab-6862a71497c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = LRPAnalyzer(np.array([split_test_data[i]['rels'] for i in split_test_data.keys()]).reshape(-1, len(feature_names)), feature_names)\n",
    "analyzer.calculate_mean_lrp_scores()\n",
    "analyzer.calculate_mean_abs_lrp_scores()\n",
    "analyzer.plot_lrp_scores(os.path.join(figure_path, 'sorted_mean_lrp_NN_GI.png'))\n",
    "analyzer.plot_abs_lrp_scores(os.path.join(figure_path, 'sorted_mean_abs_lrp_NN_GI.png'))\n",
    "analyzer.save_scores_to_csv(os.path.join(storing_path, 'sorted_mean_lrp_NN.csv'), os.path.join(storing_path, 'sorted_mean_abs_lrp_NN.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68334a1a-cddd-4b18-ad8d-a25d73d85b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect and reshape relevances for plotting\n",
    "plt_rels = np.concatenate([split_test_data[i]['rels'] for i in split_test_data.keys()], 1)\n",
    "\n",
    "plot_feature_importance_distribution(np.abs(plt_rels).mean(0), feature_names, 'NN', color='gray', savedir=figure_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35953b-8ba9-488a-be1f-0d155b4b1703",
   "metadata": {},
   "source": [
    "## Neural network without resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72202689-9b87-433f-9ad6-e186a9187f1a",
   "metadata": {},
   "source": [
    "### Explanations based on positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d394e-57cb-43f7-abcd-b25a65d463e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_resampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab7fed-69f8-472a-9834-0bc3f97d110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This explanation is based on the positive class [0,1]\n",
    "criterion = nn.BCELoss()\n",
    "acc = 0\n",
    "k = 5 # cross-validation folds\n",
    "n = 3\n",
    "n_iter = 3 \n",
    "patience = 10  \n",
    "verbose = False\n",
    "# instantiate variables for ease of use later\n",
    "artificial_neuron=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d77e80-c26c-4777-969f-cc30b5b53bb1",
   "metadata": {},
   "source": [
    "### Training and nested cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de179a-7920-424b-80e6-4a04c17feb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mlp_g_nr = []\n",
    "precision_mlp_g_nr = []\n",
    "recall_mlp_g_nr = []\n",
    "f1_mlp_g_nr = []\n",
    "\n",
    "\n",
    "selected_models_max_g_nr = defaultdict(list)  # For storing the selected model architecture and its F1 score for each split\n",
    "selected_models_g_nr = defaultdict(list)  # For storing the all model architecture and its F1 score for each split\n",
    "selected_models_counts_g_nr = Counter()  # For storing counts and F1 scores of selected models across splits\n",
    "selected_models_nr = [] # store best models as objects\n",
    "split_test_data_nr = {}  # Store confusion information and relevance scores for each split\n",
    "\n",
    "for rs in range(n):\n",
    "    print(f\"Split {rs}\")\n",
    "    start_time = time.time()\n",
    "    ### Get the data loaders for the current split\n",
    "\n",
    "    X_train, y_train, test_loader = get_test_data_loader(X_pos, X_neg, y_pos, y_neg, rs)\n",
    "\n",
    "    ### Iterate over all combinations of hyperparameters for given split and select the best-performing model\n",
    "    max_f1_model_nr = None  # Model with max validation F1 score\n",
    "    max_f1_val = 0  # Max validation F1 score\n",
    "\n",
    "    convergences = []\n",
    "    idx_max_f1_model = 0\n",
    "    for c, comb in enumerate(all_combs[:5]):\n",
    "        if c % 50 == 0:\n",
    "            print(f\"> Combination {c}/{len(all_combs)}\")\n",
    "\n",
    "        num_layers, num_neurons_per_layer, dropout_rate, lr, wd = comb\n",
    "\n",
    "        model = NeuralNetwork(num_layers, num_neurons_per_layer, dropout_rate, artificial_neuron=act_rel)\n",
    "        if torch.cuda.is_available() and use_gpu:\n",
    "            model = model.cuda()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), weight_decay=wd, lr=lr)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        xval_f1_scores = []\n",
    "\n",
    "        for k_i in range(k):\n",
    "            train_loader, val_loader = get_xval_data_loaders(k, k_i, X_train, y_train,\n",
    "                                                             with_resampling=with_resampling,\n",
    "                                                             verbose=verbose)\n",
    "            early_stopping_counter = 0\n",
    "            best_val_f1 = 0\n",
    "            f1_val = 0\n",
    "            iteration = 0\n",
    "\n",
    "            # iterate n_iter epochs\n",
    "            for iteration in tqdm(range(n_iter), desc=f\"Training for max {n_iter} epochs\", leave=False):\n",
    "\n",
    "                model.train()\n",
    "                train_epoch(train_loader, model, criterion, optimizer)\n",
    "\n",
    "                model.eval()\n",
    "                val_pred, val_gt = val_epoch(val_loader, model)\n",
    "        \n",
    "                f1_val = f1_score(np.array(val_gt), np.array(val_pred))\n",
    "        \n",
    "                if f1_val > best_val_f1:\n",
    "                    best_val_f1 = f1_val\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "        \n",
    "                if early_stopping_counter >= patience:\n",
    "                    break\n",
    "            xval_f1_scores.append(best_val_f1)\n",
    "\n",
    "        \n",
    "        best_val_f1 = np.mean(xval_f1_scores)\n",
    "\n",
    "        # Store epoch that the model converged\n",
    "        convergences.append(iteration)\n",
    "        # Store the selected model architecture and its F1 score for this split\n",
    "        model_architecture = str(model)  # Convert the model architecture to a string\n",
    "        selected_models_g_nr[model_architecture].append({'f1_score': f1_val, 'hyperparams': comb})\n",
    "\n",
    "        # Store the model for given combination if it has a higher F1 score\n",
    "        if best_val_f1 >= max_f1_val:\n",
    "            max_f1_model_nr = copy.deepcopy(model)\n",
    "            max_f1_val = best_val_f1\n",
    "            max_f1_comb = comb\n",
    "            idx_max_f1_model = c\n",
    "\n",
    "    ### Store model architecture that achieved the highest F1 score over all combinations for 1 split\n",
    "    model_architecture_max = str(max_f1_model_nr)  # Convert the model architecture to a string\n",
    "    selected_models_max_g_nr[model_architecture_max].append({\"f1_score\": max_f1_val, \"hyperparams\": max_f1_comb})\n",
    "    selected_models_nr.append((max_f1_val, max_f1_model_nr))\n",
    "    # Increase counter for the selected model architecture\n",
    "    selected_models_counts_g_nr[model_architecture_max] += 1\n",
    "    max_f1_model_nr.cpu()\n",
    "\n",
    "    ### Evaluate the best-performing model on the test set.\n",
    "    max_f1_model_nr.eval()\n",
    "    modified_model = ModifiedNeuralNetwork(max_f1_model_nr)\n",
    "\n",
    "    pred, gt, probs, rels, confusion_scores, confusion_idxs = modified_model.inference_with_relevance(test_loader, reweight_explanation=True,\n",
    "                                                                                               relevance_on_positive_class=True)\n",
    "    gt = np.array(gt)\n",
    "    pred = np.array(pred)\n",
    "    rels = np.array(rels)\n",
    "    probs = np.array(probs)\n",
    "\n",
    "    split_test_data_nr[rs] = {\n",
    "        'pred': pred,\n",
    "        'gt': gt,\n",
    "        'rels': rels,\n",
    "        'probs' : probs,\n",
    "        # rels is a dict, containing the keys R_on_pred and R_on_pos_cls, each of which is a list of numpy arrays\n",
    "        'confusion_scores': confusion_scores,\n",
    "        # confusion_scores is a dict, containing the keys true_pos_scores, true_neg_scores, false_pos_scores, and false_neg_scores, each of which is a list of floats\n",
    "        'confusion_idxs': confusion_idxs,\n",
    "        # confusion_idxs is a dict, containing the keys true_pos_idx, true_neg_idx, false_pos_idx, and false_neg_idx, each of which is a list of integers\n",
    "        'convergence_times': convergences,\n",
    "        # store number of epochs until convergence for each combination of hyperparameters\n",
    "        'convergence_time_max_f1_model': convergences[idx_max_f1_model]\n",
    "        # store number of epochs until convergence for the model with the highest F1 score\n",
    "    }\n",
    "\n",
    "    acc_mlp_g_nr.append(accuracy_score(gt, pred))\n",
    "    precision_mlp_g_nr.append(precision_score(gt, pred, zero_division=1))\n",
    "    recall_mlp_g_nr.append(recall_score(gt, pred))\n",
    "    f1_mlp_g_nr.append(f1_score(gt, pred))\n",
    "\n",
    "    print(f\"> Total computation time for split {rs}: {timedelta(seconds=(time.time() - start_time))}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f555b472-0e0a-45e7-82d6-fee59ec87d57",
   "metadata": {},
   "source": [
    "### Store all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca07706-37cf-4db5-948b-154468ca43bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    with open(storing_path / f'all_results_resampling_{with_resampling}.pkl', 'wb') as f:\n",
    "        pickle.dump(split_test_data_nr, f)\n",
    "        \n",
    "    with open(storing_path / f'max_f1_models_resampling_{with_resampling}.pkl', 'wb') as f:\n",
    "        pickle.dump(selected_models_max_g_nr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fe290-d20f-4a16-a7ea-eacb8a42cba1",
   "metadata": {},
   "source": [
    "### Display and save the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b58d7-a6e2-4da0-80bc-2ca2f7fe4ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metrics = get_formatted_results(acc_mlp_g_nr, f1_mlp_g_nr, precision_mlp_g_nr, recall_mlp_g_nr, 'Neural Networks', verbose=True)\n",
    "if SAVE:\n",
    "    file_path = f'mlp_metrics_NO_resampling_results_csv'\n",
    "    \n",
    "    ## SAVING ANALYSIS RESULTS\n",
    "    df_metrics.to_csv(os.path.join(storing_path, 'NN_metrics_NO_Resampling_results.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea98d6c-1697-44f1-a4dd-ea19b59ed970",
   "metadata": {},
   "source": [
    "### Signed and absolute average feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7991198-4e29-4e67-ab88-7ea5b4eee060",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = LRPAnalyzer(np.array([split_test_data_nr[i]['rels'] for i in split_test_data_nr.keys()]).reshape(-1, len(feature_names)), feature_names)\n",
    "analyzer.calculate_mean_lrp_scores()\n",
    "analyzer.calculate_mean_abs_lrp_scores()\n",
    "analyzer.plot_lrp_scores(os.path.join(figure_path, 'sorted_mean_lrp_NN_NO_Resampling_GI.png'))\n",
    "analyzer.plot_abs_lrp_scores(os.path.join(figure_path, 'sorted_mean_abs_lrp_NN_NO_Resampling_GI.png'))\n",
    "analyzer.save_scores_to_csv(os.path.join(storing_path, 'sorted_mean_lrp_NN_NO_Resampling.csv'), os.path.join(storing_path, 'sorted_mean_abs_lrp_NN_NO_Resampling.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6fca32",
   "metadata": {},
   "source": [
    "## Create single sample visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6428e218-b7b0-45c6-abab-d3282ab89d3b",
   "metadata": {},
   "source": [
    "### Select a random split to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random split from all results, i.e., random key of split_test_data dictionary\n",
    "np.random.seed(1)\n",
    "rs = np.random.choice(list(split_test_data.keys()))\n",
    "single_sample_split_rs = rs\n",
    "\n",
    "print(f\"Randomly selected split: {rs}\\n\")\n",
    "\n",
    "confusion_scores = split_test_data[rs]['confusion_scores']\n",
    "confusion_idxs = split_test_data[rs]['confusion_idxs']\n",
    "rels = split_test_data[rs]['rels']\n",
    "print('confiusion scores shape', confusion_scores.keys())\n",
    "print('rels shape', rels.shape)\n",
    "#print('Keys of different subdictionaries:')\n",
    "#print(confusion_scores.keys())\n",
    "#print(confusion_idxs.keys())\n",
    "#print(rels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ca14d-8240-4d22-9149-5c136c38a535",
   "metadata": {},
   "source": [
    "### Find outlier samples, used later for plotting single sample explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highest_and_lowest_scores(scores, idxs, top_k=1, bottom_k=1):\n",
    "    \"\"\"For each confusion category, get the highest and lowest scoring samples and their indexes.\"\"\"\n",
    "    # Convert score lists to numpy arrays for easier indexing\n",
    "    scores = np.array(scores)\n",
    "    idxs = np.array(idxs)\n",
    "\n",
    "    # Sorting indices\n",
    "    sort_idx = np.argsort(scores)\n",
    "\n",
    "    # High and low scores\n",
    "    high_scores = scores[sort_idx[-top_k:]]\n",
    "    high_idxs = idxs[sort_idx[-top_k:]]\n",
    "\n",
    "    low_scores = scores[sort_idx[:bottom_k]]\n",
    "    low_idxs = idxs[sort_idx[:bottom_k]]\n",
    "\n",
    "    return high_scores, high_idxs, low_scores, low_idxs\n",
    "\n",
    "index_lists = []\n",
    "score_lists = []\n",
    "prob_lists = []\n",
    "categories = []\n",
    "name_mapping = {\n",
    "    'true_pos_scores': 'True Positives',\n",
    "    'false_pos_scores': 'False Positives',\n",
    "    'true_neg_scores': 'True Negatives',\n",
    "    'false_neg_scores': 'False Negatives'\n",
    "}\n",
    "\n",
    "# Desired order\n",
    "desired_order = [\n",
    "    'High True Positives', 'Low True Positives',\n",
    "    'High False Positives', 'Low False Positives',\n",
    "    'High False Negatives', 'Low False Negatives',\n",
    "    'High True Negatives', 'Low True Negatives'\n",
    "]\n",
    "\n",
    "temp_categories = []\n",
    "temp_index_lists = []\n",
    "temp_score_lists = []\n",
    "temp_prob_lists = []\n",
    "\n",
    "for ((scores_key, scores), (idx_key, idxs)) in zip(confusion_scores.items(), confusion_idxs.items()):\n",
    "    high_scores, high_idxs, low_scores, low_idxs = get_highest_and_lowest_scores(scores, idxs, top_k=1, bottom_k=1)\n",
    "\n",
    "    temp_categories.append(f\"High {name_mapping[scores_key]}\")\n",
    "    temp_index_lists.append(high_idxs)\n",
    "    temp_score_lists.append(high_scores)\n",
    "\n",
    "    temp_categories.append(f\"Low {name_mapping[scores_key]}\")\n",
    "    temp_index_lists.append(low_idxs)\n",
    "    temp_score_lists.append(low_scores)\n",
    "\n",
    "# Sorting the results to match the desired order\n",
    "for category in desired_order:\n",
    "    index = temp_categories.index(category)\n",
    "    categories.append(temp_categories[index])\n",
    "    index_lists.append(temp_index_lists[index])\n",
    "    score_lists.append(temp_score_lists[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef735e-3861-40c8-be5d-90196bb6a77c",
   "metadata": {},
   "source": [
    "### Visualize the relevance scores for the highest and lowest scoring samples per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize=(8.27, 11.69) is A4 paper size -> lets make it a bit smaller\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(8, 10.95))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over all categories in the confusion matrix\n",
    "for k, (index_list, score_list, category) in enumerate(zip(index_lists, score_lists, categories)):\n",
    "    idx = index_list[0]\n",
    "    score = score_list[0]\n",
    "    # rels_single_sample = rels_list[idx]  # Is a list with feature relevance scores for given sample\n",
    "    rels_single_sample = rels[idx].squeeze()\n",
    "\n",
    "    df_single_sample = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance Score': rels_single_sample\n",
    "    }).sort_values(by='Importance Score', ascending=True)\n",
    "\n",
    "    # Find the index of the first zero value\n",
    "    zero_index = df_single_sample['Importance Score'].eq(0).idxmax()\n",
    "\n",
    "    # skip zero relevance elements to save on space for visualization\n",
    "    df_single_sample = df_single_sample[\n",
    "        ~((df_single_sample['Importance Score'] == 0) & (df_single_sample.index != zero_index))]\n",
    "    df_single_sample.loc[zero_index, 'Feature'] = '[...]'\n",
    "\n",
    "    palette = custom_palette(df_single_sample['Importance Score'])\n",
    "\n",
    "    # plot relevances for sample\n",
    "    sns.barplot(x='Importance Score', y='Feature', data=df_single_sample, palette=palette, ax=axes[k])\n",
    "    axes[k].set_title(f\"{category} - Sample {idx} (Score: {score:.2f})\", fontsize=12)\n",
    "    axes[k].set_xlabel('Importance Score')\n",
    "    axes[k].set_ylabel('Features')\n",
    "    # fig.suptitle(\n",
    "    #     f\"Feature Importance Scores for Highest and Lowest Scoring Samples\\n(Relevances computed w.r.t. {'prediction' if 'pred' in rel_key else 'the positive class'})\",\n",
    "    #     fontsize=13)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "# save figure\n",
    "if SAVE:\n",
    "    plt.savefig(\n",
    "        figure_path / f\"LRP_on_pos_class_samples_per_cat.png\",\n",
    "        dpi=300, facecolor=(1, 1, 1, 0),\n",
    "        bbox_inches='tight')\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a55929",
   "metadata": {},
   "source": [
    "## Generating catalyst candidates using LRP importances for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fd928-eeeb-4518-8209-fa1ad68a8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load feature importances\n",
    "df_feature_importance_nn = pd.read_csv(os.path.join(storing_path, 'sorted_mean_lrp_NN.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a41efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "el_factor = 20\n",
    "supp_factor = 2\n",
    "print(df_feature_importance_nn)\n",
    "\n",
    "candidates = generate_catalysts_from_relevance_scores(df_feature_importance_nn['Importance Score'].to_numpy(),\n",
    "                                                      df_feature_importance_nn['Feature'].to_list(),\n",
    "                                                      num_candidates=1000,\n",
    "                                                      elem_importance_factor=el_factor,\n",
    "                                                      supp_importance_factor=supp_factor,\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd8ecf-160f-4171-947b-03c85735c39e",
   "metadata": {},
   "source": [
    "### Convert candidates to numpy features, remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35832ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = catalyst_string_to_numpy(candidates,\n",
    "                                    df_feature_importance_rebalanced['Feature'].to_list(),\n",
    "                                    remove_duplicates=True)\n",
    "\n",
    "# classify with a single model to see the fraction of samples that would be classified as high-yield\n",
    "logits = max_f1_model(torch.from_numpy(feats).float())\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)  # Get the predicted class\n",
    "#print('prediction', y_pred)\n",
    "print('fraction of samples predicted as true', float(torch.sum(y_pred) / y_pred.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807114a-014f-4d34-b135-69547ce9fc91",
   "metadata": {},
   "source": [
    "### Remove diplicates that appear in OCM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c61c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate([X_pos,X_neg], axis=0)\n",
    "\n",
    "feats_view = np.ascontiguousarray(feats).view([('', feats.dtype)] * feats.shape[1])\n",
    "X_view = np.ascontiguousarray(X).view([('', X.dtype)] * X.shape[1])\n",
    "\n",
    "# Perform set difference on the rows\n",
    "feats_diff = np.setdiff1d(feats_view, X_view)\n",
    "\n",
    "# Convert back to the original array format\n",
    "feats_new = feats_diff.view(feats.dtype).reshape(-1, feats.shape[1])\n",
    "\n",
    "print(\"Unique generated candidates that do not appear in training set\", feats_new.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922913e6-286f-4a7e-9f46-e15a06cc4af7",
   "metadata": {},
   "source": [
    "### Select the top N neural network models from the set of best models of each split in order to evaluate catalyst candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d671d086-4a61-4e98-a271-8be94677b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_models = 3\n",
    "selected_model_scores = np.array([selected_models[i][0] for i in range(len(selected_models))])\n",
    "best_scores_idx = np.argsort(-selected_model_scores)[:N_models]\n",
    "best_scores = [selected_models[i][0] for i in best_scores_idx]\n",
    "best_models = [selected_models[i][1] for i in best_scores_idx]\n",
    "print(\"Sorted scores of best models from each split\", best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee1da6-93d0-42b8-bf0e-d8466ae804b1",
   "metadata": {},
   "source": [
    "### Select the top 20 candidates based on the average scores from the NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff70dd-deab-4be9-90dc-2091a51cd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_logits = torch.stack([max_model(torch.from_numpy(feats_new).float()) for max_model in best_models], dim=0)\n",
    "\n",
    "logits = all_logits.mean(dim=0)\n",
    "\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "all_pred_probab = nn.Softmax(dim=-1)(all_logits)\n",
    "\n",
    "top_probab_idx = np.argsort(-(pred_probab[:, 1]).numpy(force=True))\n",
    "top_prob_feats = feats_new[top_probab_idx[:20]]\n",
    "\n",
    "cand_new = numpy_to_catalyst_string(top_prob_feats, feature_names)\n",
    "print('List of top 20 generated promising catalyst candidates', cand_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebff841-fcc3-40a2-b6eb-6ebb305902f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

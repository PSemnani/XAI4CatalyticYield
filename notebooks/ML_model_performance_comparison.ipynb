{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e35b7e-c35d-4faf-9ddd-c21e7b637eed",
   "metadata": {},
   "source": [
    "# Compare classification performance for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de506d1b-ecd5-4b70-9d9c-253c3cf4644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from skopt.plots import plot_convergence\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import xgboost.sklearn as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from itertools import product\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "from mlxai4cat.utils.data import prepare_dataset, stratified_sampling, resampling \n",
    "from mlxai4cat.utils.visualization import get_formatted_results, plot_feature_importance, plot_feature_importance_distribution\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"skopt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2738c5-83cd-4e56-add4-149831ce980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444f52f-fb18-4525-b2f4-8775886c634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "storing_path = Path('../results')\n",
    "figure_path = Path('../figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb7bd0-222e-4eb3-8857-970db8facf29",
   "metadata": {},
   "source": [
    "## Load performance metrics of all models and combine in single DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e445e2-8b91-4b8d-a718-6b5e5773edb9",
   "metadata": {},
   "source": [
    "### With resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c19d588-1df6-460a-9f6d-43ab09a31cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt_metrics = pd.read_csv(os.path.join(storing_path, 'DT_metrics_results.csv'))\n",
    "                         \n",
    "df_rf_metrics = pd.read_csv(os.path.join(storing_path, 'RF_metrics_results.csv'))\n",
    "                         \n",
    "df_nn_metrics = pd.read_csv(os.path.join(storing_path, 'NN_metrics_results.csv'))\n",
    "\n",
    "df_lr_metrics = pd.read_csv(os.path.join(storing_path, 'LR_metrics_results.csv'))\n",
    "                            \n",
    "df_svm_metrics = pd.read_csv(os.path.join(storing_path, 'SVM_metrics_results.csv'))\n",
    "\n",
    "df_metrics = pd.concat([df_dt_metrics, df_rf_metrics, df_nn_metrics, df_lr_metrics, df_svm_metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cc764-2f8c-479f-9128-4baf79eac444",
   "metadata": {},
   "source": [
    "### Without resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c876c0d-7c94-4f0b-8575-d29453406cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt_metrics_nr = pd.read_csv(os.path.join(storing_path, 'DT_metrics_NO_Resampling_results.csv'))\n",
    "                         \n",
    "df_rf_metrics_nr = pd.read_csv(os.path.join(storing_path, 'RF_metrics_NO_Resampling_results.csv'))\n",
    "                         \n",
    "df_nn_metrics_nr = pd.read_csv(os.path.join(storing_path, 'NN_metrics_NO_Resampling_results.csv'))\n",
    "\n",
    "df_lr_metrics_nr = pd.read_csv(os.path.join(storing_path, 'LR_metrics_NO_Resampling_results.csv'))\n",
    "                            \n",
    "df_svm_metrics_nr = pd.read_csv(os.path.join(storing_path, 'SVM_metrics_NO_Resampling_results.csv'))\n",
    "\n",
    "df_metrics_nr = pd.concat([df_dt_metrics_nr, df_rf_metrics_nr, df_nn_metrics_nr, df_lr_metrics_nr, df_svm_metrics_nr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0117c7-79ee-4a52-a2b6-aab19c6a9e92",
   "metadata": {},
   "source": [
    "## Converting the df_metrics and saving it for exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25295716-edee-4cf0-a154-53df6dc6417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_accandF1 = df_metrics[['Model', 'Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']]\n",
    "\n",
    "# Round the numeric columns to two decimal places\n",
    "df_metrics_accandF1[['Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']] = df_metrics_accandF1[['Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']].round(2)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "file_path = os.path.join(storing_path, 'Model_Comparison_accuracyandF1.csv')  # Specify the file path\n",
    "df_metrics_accandF1.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c4e54-f067-4900-9b42-dad925f3e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_complete = df_metrics.copy()\n",
    "\n",
    "# Exclude the first column (assuming it's non-numeric)\n",
    "numeric_cols = df_metrics_complete.columns[1:]\n",
    "\n",
    "# Convert the numeric columns to float type\n",
    "df_metrics_complete[numeric_cols] = df_metrics_complete[numeric_cols].astype(float)\n",
    "\n",
    "# Round the values in the numeric columns to two decimal places\n",
    "df_metrics_complete[numeric_cols] = df_metrics_complete[numeric_cols].round(2)\n",
    "\n",
    "# Print the rounded DataFrame\n",
    "print(df_metrics_complete)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = os.path.join(storing_path, 'data_for_model_comparision_complete.csv')\n",
    "\n",
    "# Save the rounded DataFrame to a CSV file\n",
    "df_metrics_complete.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c165644-c5e7-477b-96ad-5e4bb64570c3",
   "metadata": {},
   "source": [
    "## Plot the performance results for models with resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fcb74f-c2df-4a56-9672-fbb9950642d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_models = ['Decision Tree','DT prepruned', 'DT postpruned', 'Random forest', 'XGBoost', 'Logistic regression','SVM', 'Neural Networks']\n",
    "\n",
    "# Filter the dataframe to include only the selected models\n",
    "df_selected_metrics = df_metrics[df_metrics['Model'].isin(selected_models)]\n",
    "\n",
    "# Define colors for each metric\n",
    "#colors = ['saddlebrown', 'olivedrab', 'rosybrown', 'gray' ]\n",
    "#colors = ['#377eb8', '#ff7f00', '#4daf4a', '#984ea3']\n",
    "colors = ['#c0c0c0', '#3cb371', '#ffc1a1', '#b0c4de']\n",
    "\n",
    "# Extract data for plotting\n",
    "models = df_selected_metrics['Model']\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "mean_columns = [f'{metric}_Mean' for metric in metrics]\n",
    "std_columns = [f'{metric}_Std' for metric in metrics]\n",
    "mean_values = df_selected_metrics[mean_columns].values\n",
    "std_values = df_selected_metrics[std_columns].values\n",
    "\n",
    "# Plotting\n",
    "bar_width = 0.2\n",
    "opacity = 0.8\n",
    "index = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_data = mean_values[:, i]\n",
    "    std_data = std_values[:, i]\n",
    "    ax.bar(index + i * bar_width, mean_data, bar_width,\n",
    "           alpha=opacity, label=f'{metric}_Mean', yerr=std_data, capsize=5, color=colors[i])\n",
    "\n",
    "# Adjust font sizes and labels for selected models\n",
    "fontsize = 14\n",
    "ax.set_xlabel('Models', fontsize=16)\n",
    "ax.set_ylabel('Performance Metrics', fontsize=16)\n",
    "#ax.set_title('Comparison of Model Performance Metrics', fontsize=fontsize)\n",
    "ax.set_xticks(index + (bar_width * (len(metrics) - 1)) / 2)\n",
    "ax.set_xticklabels(['Decision Tree','DT prepruned', 'DT postpruned', 'Random Forest', 'XGBoost', 'Logistic Regression', 'SVM', 'Neural Networks'], fontsize=fontsize)\n",
    "plt.yticks(fontsize=14)\n",
    "ax.legend(fontsize=fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(figure_path, 'Model_Performance_Comparision_with_Resampling_new.png'), facecolor=(1,1,1,0), bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190958bf-b1ca-447c-b4dc-aa82ab03ead2",
   "metadata": {},
   "source": [
    "## Plot the performance results for models with no resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa41d54e-fdfd-4176-8178-f167b111ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#c0c0c0', '#3cb371', '#ffc1a1', '#b0c4de']\n",
    "models = df_metrics_nr['Model']\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "mean_columns = [f'{metric}_Mean' for metric in metrics]\n",
    "std_columns = [f'{metric}_Std' for metric in metrics]\n",
    "mean_values = df_metrics_nr[mean_columns].values\n",
    "std_values = df_metrics_nr[std_columns].values\n",
    "\n",
    "# Plotting\n",
    "bar_width = 0.2\n",
    "opacity = 0.8\n",
    "index = np.arange(len(models))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    mean_data = mean_values[:, i]\n",
    "    std_data = std_values[:, i]\n",
    "    ax.bar(index + i * bar_width, mean_data, bar_width,\n",
    "           alpha=opacity, label=f'{metric}_Mean', yerr=std_data, capsize=5, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Performance Metrics')\n",
    "ax.set_title('Comparison of Model Performance Metrics without resampling')\n",
    "ax.set_xticks(index + (bar_width * (len(metrics) - 1)) / 2)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(os.path.join(figure_path, 'Model_Performance_Comparision_without_Resampling_new.png'), facecolor=(1,1,1,0), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b13ad33-0e23-42fa-9d9e-94d33adc48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_accandF1_nr = df_metrics_nr[['Model', 'Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']]\n",
    "\n",
    "# Round the numeric columns to two decimal places\n",
    "df_metrics_accandF1_nr[['Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']] = df_metrics_accandF1_nr[['Accuracy_Mean', 'Accuracy_Std' ,'F1_Mean', 'F1_Std']].round(2)\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "file_path = os.path.join(storing_path, 'Model_Comparison_accuracyandF1_nr_new.csv')  # Specify the file path\n",
    "df_metrics_accandF1_nr.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b499ad-4f48-4c15-b2cf-1076cc5b94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in df_metrics_nr to distinguish from df_metrics\n",
    "df_metrics_nr_renamed = df_metrics_nr.rename(columns={\n",
    "    'Accuracy_Mean': 'Accuracy_Mean_nr',\n",
    "    'Accuracy_Std': 'Accuracy_Std_nr',\n",
    "    'F1_Mean': 'F1_Mean_nr',\n",
    "    'F1_Std': 'F1_Std_nr',\n",
    "    'Precision_Mean': 'Precision_Mean_nr',\n",
    "    'Precision_Std': 'Precision_Std_nr',\n",
    "    'Recall_Mean': 'Recall_Mean_nr',\n",
    "    'Recall_Std': 'Recall_Std_nr'\n",
    "})\n",
    "\n",
    "# Merge the two DataFrames on the 'Model' column\n",
    "merged_df = pd.merge(df_metrics, df_metrics_nr_renamed, on='Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3b59b-c24f-4d90-8aae-246622e4bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "metrics = ['Accuracy', 'F1', 'Precision', 'Recall']\n",
    "\n",
    "# Create individual plots for each metric\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(16, 12))\n",
    "\n",
    "colors = ['#c0c0c0', '#3cb371', '#ffc1a1', '#b0c4de']\n",
    "# Reshape axes to fit the new layout\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Extract relevant columns\n",
    "    mean_metrics_column = f'{metric}_Mean'\n",
    "    std_metrics_column = f'{metric}_Std'\n",
    "    mean_metrics_nr_column = f'{metric}_Mean_nr'\n",
    "    std_metrics_nr_column = f'{metric}_Std_nr'\n",
    "\n",
    "    # Calculate the differences\n",
    "    mean_diff = merged_df[mean_metrics_column] - merged_df[mean_metrics_nr_column]\n",
    "\n",
    "    # Create a bar plot with error bars\n",
    "    models = merged_df['Model']\n",
    "    index = np.arange(len(models))\n",
    "\n",
    "    bars = axes[i].bar(index, mean_diff, alpha=0.7, label=f'{metric} Difference', color=colors[i])\n",
    "\n",
    "    axes[i].set_xlabel('Models')\n",
    "    axes[i].set_ylabel(f'{metric} Difference')\n",
    "    axes[i].set_title(f'Difference in {metric} Mean with and without Resampling')\n",
    "    axes[i].set_xticks(index)\n",
    "    axes[i].set_xticklabels(models, rotation=45, ha='right')  # Rotate x-axis labels for better visibility\n",
    "    axes[i].legend()\n",
    "\n",
    "    # Add grid lines for better readability\n",
    "    axes[i].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(figure_path, 'Difference_between_Metrics_with_and_without_Resampling.png'), facecolor=(1,1,1,0), bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
